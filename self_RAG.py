import re 
import json
import time 
from typing import List
from typing_extensions import TypedDict, Literal

from config import Config
from make_collection import get_index

from IPython.display import Image, display
import streamlit as st
from langchain import hub
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langgraph.graph import END, StateGraph, START
from langchain_community.retrievers.llama_index import LlamaIndexRetriever
# from langchain_community.retrievers.llama_index import LlamaIndexGraphRetriever
from langchain.schema import Document
from pydantic import BaseModel, Field

from templates import (
                    prompt,
                    grade_prompt, 
                    hallucination_prompt, 
                    answer_prompt,
                    re_write_prompt,
                    GradeAnswer,
                    GradeDocuments,
                    GradeHallucinations)


st.set_page_config(
    page_title="æ™ºèƒ½åŠ³åŠ¨æ³•å’¨è¯¢åŠ©æ‰‹",
    page_icon="âš–ï¸",
    layout="centered",
    initial_sidebar_state="auto"
)

def disable_streamlit_watcher():
    """ä¿®è¡¥ Streamlitï¼Œç¦ç”¨æ–‡ä»¶ç›‘è§†å™¨"""
    def _on_script_changed(_):
        return
        
    from streamlit import runtime
    runtime.get_instance()._on_script_changed = _on_script_changed
    
    
# ========= åˆå§‹åŒ–ç¨‹åº ==========
config = Config()
embed_model, llm, reranker,index = get_index(config)

retriever = index.as_retriever(similarity_top_k=Config.TOP_K,vector_store_query_mode="hybrid",alpha=0.5)



llm_doc_wso = llm.with_structured_output(GradeDocuments)
llm_answer_wso = llm.with_structured_output(GradeAnswer)
llm_hallucination_wso = llm.with_structured_output(GradeHallucinations)

llm_origin = prompt | llm  # ç”¨äºç”Ÿæˆç­”æ¡ˆ
llm_rewrite = re_write_prompt | llm  # ç”¨äºé—®é¢˜é‡å†™
llm_doc = grade_prompt | llm_doc_wso  # ç”¨äºæ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†
llm_answer = answer_prompt | llm_answer_wso  # ç”¨äºåˆ¤æ–­æ˜¯å¦è§£å†³é—®é¢˜
llm_hallucination = hallucination_prompt | llm_hallucination_wso  # ç”¨äºå¹»è§‰æ£€æµ‹ 

# ======== å®šä¹‰ GraphState å’Œå„èŠ‚ç‚¹å‡½æ•° ==========
class GraphState(TypedDict):
    """
    è¡¨ç¤ºå›¾çš„çŠ¶æ€ã€‚

    å±æ€§:
        question: ç”¨æˆ·çš„é—®é¢˜ã€‚
        generation: LLM ç”Ÿæˆçš„ç­”æ¡ˆã€‚
        documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ã€‚
    """
    question: str
    generation: str
    documents: List[Document]

def retrieve_node(state: GraphState)->GraphState:
    """
    æ£€ç´¢ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„æ³•å¾‹æ¡æ¬¾ï¼Œå¹¶è¿›è¡Œé‡æ’åº

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çŠ¶æ€ã€‚

    è¿”å›:
        GraphState: æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‚
    """
    question = state["question"]
    
    # åˆçº§æ£€ç´¢
    # print(type(question),question)
    initial_nodes = retriever.retrieve(question)
    reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=question)
  
    # è¿‡æ»¤ä½åˆ†ï¼Œå¹¶å°† nodes è½¬ä¸º Document 
    documents = []
    for node in reranked_nodes:
        if node.score >= config.MIN_RERANK_SCORE:
            doc = Document(
                page_content=node.get_content(),
                metadata=node.metadata
            )
            documents.append(doc)
            
    return {"documents": documents, "question": question}
     
def generate_node(state: GraphState) -> GraphState:
    """
    ä½¿ç”¨ RAG é“¾ç”Ÿæˆç­”æ¡ˆã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çŠ¶æ€ã€‚

    è¿”å›:
        GraphState: æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å«ç”Ÿæˆçš„ç­”æ¡ˆã€‚
    """
    # print("---GENERATE---")
    question = state["question"]
    documents = state["documents"]

    # ä½¿ç”¨ RAG é“¾ç”Ÿæˆç­”æ¡ˆ
    generation = llm_origin.invoke({"context": documents, "question": question})
    # print(f"---GENERATION---\n{generation.content}\n")
    return {"documents": documents, "question": question, "generation": generation.content}

def grade_documents_node(state: GraphState) -> GraphState:
    """
    å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸é—®é¢˜çš„ç›¸å…³æ€§è¿›è¡Œè¯„åˆ†ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çŠ¶æ€ã€‚

    è¿”å›:
        GraphState: æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å«è¿‡æ»¤åçš„ç›¸å…³æ–‡æ¡£ã€‚
    """
    # print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
    question = state["question"]
    documents = state["documents"]

    # æ ¹æ®ç›¸å…³æ€§è¿‡æ»¤æ–‡æ¡£
    filtered_docs = []
    for d in documents:
        score = llm_doc.invoke(
            {"question": question, "document": d.metadata['full_title']+d.page_content}
        )
        grade = score.binary_score
        if grade == "yes":
            # print("---GRADE: DOCUMENT RELEVANT---")
            filtered_docs.append(d)
        else:
            # print("---GRADE: DOCUMENT NOT RELEVANT---")
            continue
    return {"documents": filtered_docs, "question": question}

def transform_query_node(state: GraphState) -> GraphState:
    """
    å°†ç”¨æˆ·çš„é—®é¢˜é‡å†™ä¸ºæ›´é€‚åˆæ£€ç´¢çš„ç‰ˆæœ¬ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çŠ¶æ€ã€‚

    è¿”å›:
        GraphState: æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å«é‡å†™åçš„é—®é¢˜ã€‚
    """
    # print("---TRANSFORM QUERY---")
    question = state["question"]
    documents = state["documents"]

    # é‡å†™é—®é¢˜ä»¥ä¾¿æ›´å¥½æ£€ç´¢
    better_question = llm_rewrite.invoke({"question": question})
    return {"documents": documents, "question": better_question}

def decide_to_generate(state: GraphState) -> Literal["transform_query_node", "generate_node"]:
    """
    å†³å®šæ˜¯ç”Ÿæˆç­”æ¡ˆè¿˜æ˜¯é‡å†™é—®é¢˜ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çŠ¶æ€ã€‚

    è¿”å›:
        Literal["transform_query_node", "generate_node"]: ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„å†³ç­–ã€‚
    """
    # print("---ASSESS GRADED DOCUMENTS---")
    filtered_documents = state["documents"]

    if not filtered_documents:
        # å¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œåˆ™é‡å†™é—®é¢˜
        # print("---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---")
        return "transform_query_node"
    else:
        # å¦‚æœæœ‰ç›¸å…³æ–‡æ¡£ï¼Œåˆ™ç”Ÿæˆç­”æ¡ˆ
        # print("---DECISION: GENERATE---")
        return "generate_node"
     
def decide_generation_useful(state: GraphState) -> Literal["generate_node", "transform_query_node", END]:
    """
    åˆ¤æ–­ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦æœ‰ç”¨ï¼Œæˆ–æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çŠ¶æ€ã€‚

    è¿”å›:
        Literal["generate_node", "transform_query_node", END]: ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„å†³ç­–ã€‚
    """
    # print("---CHECK HALLUCINATIONS---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]

    # æ£€æŸ¥ç”Ÿæˆå†…å®¹æ˜¯å¦åŸºäºæ–‡æ¡£
    score = llm_hallucination.invoke({"documents": documents, "generation": generation})
    grade = score.binary_score

    if grade == "yes":
        # print("---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---")
        # æ£€æŸ¥ç”Ÿæˆå†…å®¹æ˜¯å¦å›ç­”äº†é—®é¢˜
        # print("---GRADE GENERATION vs QUESTION---")
        score = llm_answer.invoke({"question": question, "generation": generation})
        grade = score.binary_score
        if grade == "yes":
            # print("---DECISION: GENERATION ADDRESSES QUESTION---")
            return END
        else:
            # print("---DECISION: GENERATION DOES NOT ADDRESS QUESTION---")
            return "transform_query_node"
    else:
        # print("---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---")
        return "generate_node"

# è¾…åŠ©å‡½æ•°ï¼Œå°†ä¸å¯åºåˆ—åŒ–å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
def convert_to_serializable(obj):
    if hasattr(obj, "dict"):              # æ£€æŸ¥å¯¹è±¡æ˜¯å¦æœ‰ .dict() æ–¹æ³•
        return obj.dict()
    elif isinstance(obj, (list, tuple)):  # å¤„ç†åˆ—è¡¨å’Œå…ƒç»„
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, dict):           # å¤„ç†å­—å…¸
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    else:                                 # å¦‚æœå·²å¯åºåˆ—åŒ–åˆ™ç›´æ¥è¿”å›
        return obj

# ================== ç•Œé¢ç»„ä»¶ ==================
def init_chat_interface():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg.get("cleaned", msg["content"])  # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹
        
        with st.chat_message(role):
            st.markdown(content)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«æ€ç»´é“¾
            if role == "assistant" and msg.get("think"):
                with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆå†å²å¯¹è¯ï¼‰"):
                    for think_content in msg["think"]:
                        st.markdown(f'<span style="color: #808080">{think_content.strip()}</span>',
                                  unsafe_allow_html=True)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”æœ‰å‚è€ƒä¾æ®ï¼ˆéœ€è¦ä¿æŒåŸæœ‰å‚è€ƒä¾æ®é€»è¾‘ï¼‰
            if role == "assistant" and "reference_nodes" in msg:
                show_reference_details(msg["reference_nodes"])

def show_reference_details(docs):
    with st.expander("æŸ¥çœ‹æ”¯æŒä¾æ®"):
        for idx, documnet in enumerate(docs, 1):
            meta = documnet.metadata
            st.markdown(f"**[{idx}] {meta['full_title']}**")
            st.caption(f"æ¥æºæ–‡ä»¶ï¼š{meta['source_file']} | æ³•å¾‹åç§°ï¼š{meta['law_name']}")
            # st.markdown(f"ç›¸å…³åº¦ï¼š`{documnet.score:.4f}`")
            # st.info(f"{node.node.text[:300]}...")
            st.info(f"{documnet.page_content[:300]}...")
            
            
if __name__ == "__main__":
    disable_streamlit_watcher()
    st.title("âš–ï¸ æ™ºèƒ½åŠ³åŠ¨æ³•å’¨è¯¢åŠ©æ‰‹")
    st.markdown("æ¬¢è¿ä½¿ç”¨åŠ³åŠ¨æ³•æ™ºèƒ½å’¨è¯¢ç³»ç»Ÿï¼Œè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†åŸºäºæœ€æ–°åŠ³åŠ¨æ³•å¾‹æ³•è§„ä¸ºæ‚¨è§£ç­”ã€‚")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "history" not in st.session_state:
        st.session_state.history = []
        # æ„å»ºå›¾
        
    
    workflow = StateGraph(GraphState)

    # å®šä¹‰èŠ‚ç‚¹
    workflow.add_node("retrieve_node", retrieve_node)                # æ£€ç´¢æ–‡æ¡£
    workflow.add_node("grade_documents_node", grade_documents_node)  # æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†
    workflow.add_node("generate_node", generate_node)                # ç”Ÿæˆç­”æ¡ˆ
    workflow.add_node("transform_query_node", transform_query_node)  # é‡å†™é—®é¢˜

    # æ„å»ºå›¾çš„è¾¹
    workflow.add_edge(START, "retrieve_node")
    workflow.add_edge("retrieve_node", "grade_documents_node")
    workflow.add_conditional_edges("grade_documents_node", decide_to_generate, ["transform_query_node", "generate_node"])
    workflow.add_edge("transform_query_node", "retrieve_node")
    workflow.add_conditional_edges("generate_node", decide_generation_useful, ["generate_node", "transform_query_node", END])

    # ç¼–è¯‘å·¥ä½œæµ
    app = workflow.compile()
    # inputs = {"question": f"{prompt}"}
    # for output in app.stream(inputs):
    #     for key, value in output.items():
    #         print(key)
                        
    # # å¯è§†åŒ–å›¾ï¼ˆå¯é€‰ï¼Œéœ€è¦é¢å¤–ä¾èµ–ï¼‰
    # try:
    #     display(Image(app.get_graph(xray=True).draw_mermaid_png('./image.png')))
    # except Exception:
    #     pass
    
    init_chat_interface()
    
    if prompt := st.chat_input("è¯·è¾“å…¥åŠ³åŠ¨æ³•ç›¸å…³é—®é¢˜"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜..."):
            start_time = time.time()
            inputs = {"question": f"{prompt}"}
            # response_text = app.invoke(inputs)
            end_time = time.time()
            list_doc = []
            generate_list = []
            for output in app.stream(inputs):
                for key, value in output.items():
                    if "generation" in value:
                        generate_list.append(value['generation'])
                    if "documents" in value :
                        list_doc.append(value['documents'])
                    
            print(list_doc)
            print(generate_list)
            response_text = generate_list[-1] if generate_list else "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆå›ç­”ã€‚" #è·å–æœ€ç»ˆå›ç­”
            filtered_docs = list_doc[-1] if list_doc else []  #è·å–æœ€ç»ˆå›ç­”æ‰€ä¾èµ–çš„æ–‡æ¡£
            
            
            with st.chat_message("assistant"):
                # æå–æ€ç»´é“¾å†…å®¹å¹¶æ¸…ç†å“åº”æ–‡æœ¬
                think_contents = re.findall(r'<think>(.*?)</think>', response_text, re.DOTALL)
                cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()
            
            # æ˜¾ç¤ºæ¸…ç†åçš„å›ç­”
            st.markdown(cleaned_response)
            
            # å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹åˆ™æ˜¾ç¤º
            if think_contents:
                with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                    for content in think_contents:
                        st.markdown(f'<span style="color: #808080">{content.strip()}</span>', 
                                    unsafe_allow_html=True)
            
            # æ˜¾ç¤ºå‚è€ƒä¾æ®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            show_reference_details(filtered_docs[:3])

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²ï¼ˆéœ€è¦å­˜å‚¨åŸå§‹å“åº”ï¼‰
            st.session_state.messages.append({
                "role": "assistant",
                "content": response_text,  # ä¿ç•™åŸå§‹å“åº”
                "cleaned": cleaned_response,  # å­˜å‚¨æ¸…ç†åçš„æ–‡æœ¬
                "think": think_contents  # å­˜å‚¨æ€ç»´é“¾å†…å®¹
            })
